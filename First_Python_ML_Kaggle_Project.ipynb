{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First_Python_ML_Kaggle_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WRff5x-3TYI"
      },
      "source": [
        "# Downloads the files to the\n",
        "# Colab instance so I don't have to upload them here.\n",
        "\n",
        "import requests\n",
        "\n",
        "def save_file(url, file_name):\n",
        "    r = requests.get(url)\n",
        "    with open(file_name, 'wb') as f:\n",
        "      f.write(r.content)\n",
        "\n",
        "save_file('https://courses.cs.washington.edu/courses/cse416/21sp/homework/hw6/edx_train.csv', \n",
        "          'edx_train.csv')\n",
        "save_file('https://courses.cs.washington.edu/courses/cse416/21sp/homework/hw6/edx_test.csv', \n",
        "          'edx_test.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHfwENUcTUOV"
      },
      "source": [
        "The following cell creates new dataframes for the given train and test set containing the original non-categorical columns along with the one-hot-encoded version of the categorical columns. The new test set contains the columns of the new train set so that the new test set can be predicted by the models, which require the test set to have the same number of columns as the train set in order to do predictions. Furthermore, I decided to use all features except for the userid_DI, course_id, start_time_DI, and last_event_DI columns because these four columns are just used for identification purposes. Ideally, they should not have anything to do with being certified (the target) since they act as identifiers that don't relate to the possible ability/potential of a person. The original train and test sets contained missing values so for the categorical columns, I filled these values with the string 'NaN', and for the non-categorical columns, I filled these values with 0. Inititally, I had also dropped the columns that contained missing values but this got rid of almost half of the total number of columns, including ones that at a glance appeared to be at least somewhat important. As a result, I decided to fill the missing values of these columns with actual values, as mentioned earlier, and found that doing so gave stronger prediction accuracies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8I5K5PO4M-u"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Reads training set and test set CSVs\n",
        "\n",
        "edx_train = pd.read_csv('edx_train.csv')\n",
        "edx_test = pd.read_csv('edx_test.csv')\n",
        "\n",
        "# Got rid of columns (features) that seemed unimportant in training and test sets\n",
        "# These columns are used for identification purposes where\n",
        "# userid_DI is only used to identify a row,\n",
        "# course_id is only used to denote a course for a row,\n",
        "# and start_time_DI and last_event_DI are denoted dates\n",
        "\n",
        "edx_train = edx_train.drop(columns = ['userid_DI',\n",
        "                                      'course_id',\n",
        "                                      'start_time_DI',\n",
        "                                      'last_event_DI',])\n",
        "edx_test = edx_test.drop(columns = ['userid_DI',\n",
        "                                    'course_id',\n",
        "                                    'start_time_DI',\n",
        "                                    'last_event_DI',])\n",
        "\n",
        "# Stores all categorical features\n",
        "\n",
        "categorical_features = ['final_cc_cname_DI','LoE_DI','gender','grade']\n",
        "\n",
        "# Fills missing values in categorical columns with the string 'NaN' in training and test sets\n",
        "# Then fills missing values in non-categorical columns with 0\n",
        "\n",
        "edx_train['final_cc_cname_DI'].fillna('NaN', inplace = True)\n",
        "edx_train['LoE_DI'].fillna('NaN', inplace = True)\n",
        "edx_train['gender'].fillna('NaN', inplace = True)\n",
        "edx_train['grade'].fillna('NaN', inplace = True)\n",
        "edx_train = edx_train.fillna(0)\n",
        "\n",
        "edx_test['final_cc_cname_DI'].fillna('NaN', inplace = True)\n",
        "edx_test['LoE_DI'].fillna('NaN', inplace = True)\n",
        "edx_test['gender'].fillna('NaN', inplace = True)\n",
        "edx_test['grade'].fillna('NaN', inplace = True)\n",
        "edx_test = edx_test.fillna(0)\n",
        "\n",
        "# Inserts a column into the test set in the place of the target column ('certified'), which wouldn't be present in test set\n",
        "# All values in this new column are 0 \n",
        "# This is so our models take in datasets with the same number of columns\n",
        "# This column will not be looked at when determining predictions\n",
        "\n",
        "edx_test.insert(3, 'certified', 0)\n",
        "\n",
        "# Creates separate datasets with one containing non-categorical features and another with categorical features\n",
        "# Does this for both the training and test set\n",
        "# This is so when we one-hot-encode, only the categorical features will be affected\n",
        "\n",
        "edx_train_not_categorical = edx_train.drop(columns=categorical_features)\n",
        "edx_test_not_categorical = edx_test.drop(columns=categorical_features)\n",
        "edx_train_categorical = edx_train[categorical_features]\n",
        "edx_test_categorical = edx_test[categorical_features]\n",
        "\n",
        "# One-Hot-Encodes the categorical features of the training and test sets so that the computer can understand the columns\n",
        "# Fits the training set so that both the training set and test set have the same one-hot-encoded columns\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown = 'ignore')\n",
        "encoder.fit(edx_train_categorical)\n",
        "edx_train_categorical = encoder.transform(edx_train_categorical).toarray()\n",
        "edx_test_categorical = encoder.transform(edx_test_categorical).toarray()\n",
        "\n",
        "# Converts one-hot-encoded datasets with the categorical features to DataFrame containing initial column names\n",
        "\n",
        "column_names = encoder.get_feature_names(categorical_features)\n",
        "edx_train_categorical = pd.DataFrame(edx_train_categorical, columns=column_names, index=edx_train.index)\n",
        "edx_test_categorical = pd.DataFrame(edx_test_categorical, columns=column_names, index=edx_test.index)\n",
        "\n",
        "# Combines the one-hot-endoded dataset with the categorical features and the regular dataset with the non-categorical features\n",
        "# Does this for both the training and test set\n",
        "# Creates new training and test datasets where the categorical features are one-hot-encoded\n",
        "\n",
        "edx_train = pd.concat([edx_train_categorical,edx_train_not_categorical],axis=1)\n",
        "edx_test = pd.concat([edx_test_categorical,edx_test_not_categorical],axis=1)\n",
        "\n",
        "# Stores the name of the target column and the features \n",
        "\n",
        "target = 'certified'\n",
        "features = list(edx_train.columns)\n",
        "features.remove('certified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvs4RUypX26a"
      },
      "source": [
        "The following cell creates a train and validation set from the original train set, which will be used by the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKov0f784YNO"
      },
      "source": [
        "# Splits training set into a new train set and validation set\n",
        "# Validation set is 20% of the original train set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, validation_data = train_test_split(edx_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02mImiHiXceS"
      },
      "source": [
        "The following cell creates a majority class classifer that essentially predicts the label for each row as the most frequent label in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F55DaDmmFI7J"
      },
      "source": [
        "# Creates a majority class classifier that \"trains\" on the training set by finding the most frequent label in the training set\n",
        "# and \"labels\" all rows as that most frequent label\n",
        "# Stores the training and validation accuracies\n",
        "\n",
        "majority_label = train_data['certified'].mode()[0]\n",
        "\n",
        "train_num_total = len(train_data)\n",
        "train_num_majority_label = len(train_data[train_data['certified'] == majority_label])\n",
        "majority_classifier_train_accuracy = train_num_majority_label/train_num_total\n",
        "\n",
        "validation_num_total = len(validation_data)\n",
        "validation_num_majority_label = len(validation_data[validation_data['certified'] == majority_label])\n",
        "majority_classifier_validation_accuracy = validation_num_majority_label/validation_num_total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSS3Aik_cY2N"
      },
      "source": [
        "The following cell creates a decision tree model with the optimal minimum number of samples in a leaf out of 1, 10, 50, 100, 200, and 300 and max depth out of 1, 5, 10, 15, 20, 25, 30, 35, and 40. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3S_P2J__HR2",
        "outputId": "9e4efb50-b649-4c5a-cc8b-00a5e9b95ed5"
      },
      "source": [
        "# Creates a decision tree classifier called 'decision_tree' that utilizes GridSearch using 6-fold validation\n",
        "# to find the optimal minimum number of samples in a leaf and optimal maximum depth\n",
        "\n",
        "hyperparameters = {'min_samples_leaf': [1,10,50,100,200,300],\n",
        "                   'max_depth': [1,5,10,15,20,25,30,35,40]}\n",
        "decision_tree = GridSearchCV(DecisionTreeClassifier(),cv=6,param_grid=hyperparameters,return_train_score=True)\n",
        "decision_tree.fit(train_data[features],train_data[target])\n",
        "\n",
        "# Stores the train and validation accuracies of the decision tree called 'decision_tree'\n",
        "\n",
        "decision_tree_train_accuracy = accuracy_score(decision_tree.predict(train_data[features]),train_data[target])\n",
        "decision_tree_val_accuracy = accuracy_score(decision_tree.predict(validation_data[features]),validation_data[target])\n",
        "\n",
        "# Prints the max depth and minimum number of samples in a leaf\n",
        "\n",
        "print(decision_tree.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_depth': 30, 'min_samples_leaf': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qg_E81Xc-ed"
      },
      "source": [
        "The following cell prints the training and validation accuracies of the majority class classifier and decision tree. The decision tree model had the highest validation accuracy, making it the model I used for my Kaggle submission. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxF03pUCQZlq",
        "outputId": "9ab03be1-9d34-4279-8cb9-8624d73906fe"
      },
      "source": [
        "# Prints the training and validation accuracies of the majority class classifier\n",
        "# and the Decision Tree model called 'decision_tree' that used GridSearch\n",
        "\n",
        "print('Majority Class Classifier')\n",
        "print(majority_classifier_train_accuracy)\n",
        "print(majority_classifier_validation_accuracy)\n",
        "\n",
        "print('Decision Tree using Grid Search')\n",
        "print(decision_tree_train_accuracy)\n",
        "print(decision_tree_val_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Majority Class Classifier\n",
            "0.5585212674850129\n",
            "0.545662100456621\n",
            "Decision Tree using Grid Search\n",
            "0.9968598344276335\n",
            "0.9920091324200914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Ovg3OndQWU"
      },
      "source": [
        "The following cell creates a new dataframe for submission containing the userid_DI column from the original test set and a predictions column containing predictions for the test set from my decision tree model. The new dataframe is then converted to a CSV file which I then downloaded and submitted to Kaggle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRXgqeHH3oeU"
      },
      "source": [
        "# Stores decision tree model predictions and converts the type to INT\n",
        "# Creates a dataframe for submission containing userid_DI and prediction columns\n",
        "# Saves dataframe to a csv file\n",
        "\n",
        "predictions = decision_tree.predict(edx_test[features])\n",
        "predictions = predictions.astype(int)\n",
        "\n",
        "df_test = pd.read_csv('edx_test.csv')\n",
        "to_save = df_test[['userid_DI']].copy()\n",
        "to_save.loc[:, 'certified'] = predictions\n",
        "to_save.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}